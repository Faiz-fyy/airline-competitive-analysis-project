{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58091fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Malaysia Airlines Competitive Analysis - Natural Language Processing\n",
    "==================================================================\n",
    "Applied NLP techniques for customer sentiment and competitive language pattern analysis.\n",
    "Scope: Text mining across 8,137 reviews with aviation-specific preprocessing and sentiment analysis\n",
    "Methods: VADER sentiment analysis, bigram network graphs, TF-IDF distinctiveness, aspect-based sentiment\n",
    "Key Features: Enhanced stopwords, sentiment-weighted networks, airline-specific language patterns\n",
    "Findings: Sentiment gap -0.154 vs industry, distinctive cultural terms, service aspect performance gaps\n",
    "Visualization: Network graphs, word clouds, TF-IDF charts, aspect sentiment comparison matrices\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652bcf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud\n",
    "import networkx as nx\n",
    "from matplotlib.patches import Patch\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84daa018",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download required NLTK data\n",
    "# Run once only then comment out (docstring), Uncomment on first run\n",
    "\"\"\"\n",
    "try:\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "except:\n",
    "    print(\"NLTK downloads may require internet connection\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abcdc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read cleaned dataset\n",
    "df = pd.read_csv(r\"CLEAN_CSV PATH HERE\") # cleaned_csv PATH here\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c281ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Advanced Text Preprocessing\n",
    "def advanced_text_preprocessing(df):\n",
    "    print(\"=== TEXT PREPROCESSING ===\")\n",
    "    \n",
    "    # Initialize NLP tools\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # Stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    airline_stopwords = {\n",
    "        'airline', 'airlines', 'flight', 'flights', 'plane', 'aircraft',\n",
    "        'airport', 'time', 'hour', 'hours', 'day', 'trip', 'travel', \n",
    "        'fly', 'flying', 'passenger', 'passengers', 'would', 'could',\n",
    "        'really', 'quite', 'very', 'much', 'many', 'also', 'well',\n",
    "        'get', 'got', 'go', 'went', 'come', 'came', 'take', 'took',\n",
    "        'one', 'two', 'first', 'last', 'back', 'way', 'class', 'business',\n",
    "        'economy', 'malaysia', 'singapore', 'qatar', 'emirates', 'airasia',\n",
    "        'kuala', 'lumpur', 'doha', 'dubai', 'changi', 'klia'\n",
    "    }\n",
    "    stop_words.update(airline_stopwords)\n",
    "    \n",
    "    def clean_text(text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def extract_bigrams(text):\n",
    "        if not text:\n",
    "            return []\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "        bigrams = list(ngrams(filtered_tokens, 2))\n",
    "        return [' '.join(bigram) for bigram in bigrams]\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    df['review_clean'] = df['review'].apply(clean_text)\n",
    "    df['bigrams'] = df['review_clean'].apply(extract_bigrams)\n",
    "    \n",
    "    # Sentiment analysis\n",
    "    sentiment_scores = df['review'].apply(lambda x: sia.polarity_scores(str(x)) if pd.notna(x) else {'compound': 0})\n",
    "    df['sentiment_score'] = sentiment_scores.apply(lambda x: x['compound'])\n",
    "    \n",
    "    def categorize_sentiment(score):\n",
    "        if score >= 0.1:\n",
    "            return 'Positive'\n",
    "        elif score <= -0.1:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "    \n",
    "    df['sentiment_category'] = df['sentiment_score'].apply(categorize_sentiment)\n",
    "    \n",
    "    print(f\"Processed {len(df):,} reviews\")\n",
    "    print(df['sentiment_category'].value_counts())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a3eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bigram Network Graph\n",
    "def create_bigram_network_graph(df):\n",
    "    print(\"\\n=== BIGRAM NETWORK GRAPH ANALYSIS ===\")\n",
    "    \n",
    "    # Focus on Malaysia Airlines for detailed analysis\n",
    "    if 'malaysia_airlines' in df['airline'].values:\n",
    "        mab_data = df[df['airline'] == 'malaysia_airlines']\n",
    "        \n",
    "        # Collect all bigrams with sentiment\n",
    "        bigram_sentiment = {}\n",
    "        bigram_frequency = Counter()\n",
    "        \n",
    "        for idx, row in mab_data.iterrows():\n",
    "            sentiment = row['sentiment_score']\n",
    "            for bigram in row['bigrams']:\n",
    "                if len(bigram) > 6:  # Filter meaningful bigrams\n",
    "                    bigram_frequency[bigram] += 1\n",
    "                    if bigram not in bigram_sentiment:\n",
    "                        bigram_sentiment[bigram] = []\n",
    "                    bigram_sentiment[bigram].append(sentiment)\n",
    "        \n",
    "        # Filter to most frequent bigrams\n",
    "        top_bigrams = {bigram: freq for bigram, freq in bigram_frequency.most_common(30)}\n",
    "        \n",
    "        # Calculate average sentiment for each bigram\n",
    "        bigram_avg_sentiment = {}\n",
    "        for bigram in top_bigrams:\n",
    "            if bigram in bigram_sentiment:\n",
    "                bigram_avg_sentiment[bigram] = np.mean(bigram_sentiment[bigram])\n",
    "        \n",
    "        # Create network graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes (individual words)\n",
    "        word_connections = defaultdict(list)\n",
    "        for bigram in top_bigrams:\n",
    "            words = bigram.split()\n",
    "            if len(words) == 2:\n",
    "                word1, word2 = words\n",
    "                G.add_edge(word1, word2, weight=top_bigrams[bigram])\n",
    "                word_connections[word1].append((word2, top_bigrams[bigram]))\n",
    "                word_connections[word2].append((word1, top_bigrams[bigram]))\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # Calculate node sizes based on total connections\n",
    "        node_sizes = {}\n",
    "        for node in G.nodes():\n",
    "            total_weight = sum(G[node][neighbor]['weight'] for neighbor in G.neighbors(node))\n",
    "            node_sizes[node] = total_weight * 100\n",
    "        \n",
    "        # Calculate node colors based on average sentiment\n",
    "        node_colors = []\n",
    "        for node in G.nodes():\n",
    "            # Find sentiment for bigrams containing this word\n",
    "            node_sentiments = []\n",
    "            for bigram, sentiment in bigram_avg_sentiment.items():\n",
    "                if node in bigram.split():\n",
    "                    node_sentiments.append(sentiment)\n",
    "            \n",
    "            if node_sentiments:\n",
    "                avg_sentiment = np.mean(node_sentiments)\n",
    "                node_colors.append(avg_sentiment)\n",
    "            else:\n",
    "                node_colors.append(0)\n",
    "        \n",
    "        # Graph layout\n",
    "        np.random.seed(42)\n",
    "        pos = nx.spring_layout(G, k=3, iterations=50)\n",
    "        \n",
    "        # Draw network\n",
    "        nodes = nx.draw_networkx_nodes(G, pos, node_size=[node_sizes.get(node, 300) for node in G.nodes()], node_color=node_colors, cmap='RdYlGn', vmin=-0.5, vmax=0.5, alpha=0.8)\n",
    "        \n",
    "        # Draw edges with thickness based on frequency\n",
    "        edges = G.edges()\n",
    "        weights = [G[u][v]['weight'] for u, v in edges]\n",
    "        nx.draw_networkx_edges(G, pos, width=[w/5 for w in weights], alpha=0.6, edge_color='gray')\n",
    "        \n",
    "        # Draw labels\n",
    "        nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "        \n",
    "        plt.title('Malaysia Airlines: Customer Language Network\\n(Node size=frequency, Color=sentiment)', fontsize=16, fontweight='bold', pad=20)\n",
    "        \n",
    "        # Add colorbar\n",
    "        sm = plt.cm.ScalarMappable(cmap='RdYlGn', norm=plt.Normalize(vmin=-0.5, vmax=0.5))\n",
    "        sm.set_array([])\n",
    "        cbar = plt.colorbar(sm, ax=plt.gca())\n",
    "        cbar.set_label('Average Sentiment', fontweight='bold')\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print top insights\n",
    "        print(\"=== TOP CONNECTED WORD PAIRS (Malaysia Airlines) ===\")\n",
    "        for bigram, freq in bigram_frequency.most_common(10):\n",
    "            sentiment = bigram_avg_sentiment.get(bigram, 0)\n",
    "            sentiment_label = \"Positive\" if sentiment > 0.1 else \"Negative\" if sentiment < -0.1 else \"Neutral\"\n",
    "            print(f\"  '{bigram}': {freq} mentions ({sentiment_label})\")\n",
    "    \n",
    "    return bigram_frequency, bigram_avg_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9694c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF Weighted Bar Charts\n",
    "def create_tfidf_analysis(df):\n",
    "    print(\"\\n=== TF-IDF IMPORTANCE ANALYSIS ===\")\n",
    "    \n",
    "    # Prepare documents by airline\n",
    "    airline_documents = {}\n",
    "    airline_sentiments = {}\n",
    "    \n",
    "    for airline in df['airline'].unique():\n",
    "        airline_data = df[df['airline'] == airline]\n",
    "        # Combine all reviews for this airline\n",
    "        combined_text = ' '.join(airline_data['review_clean'].fillna(''))\n",
    "        airline_documents[airline] = combined_text\n",
    "        # Calculate average sentiment\n",
    "        airline_sentiments[airline] = airline_data['sentiment_score'].mean()\n",
    "    \n",
    "    # TF-IDF Analysis\n",
    "    vectorizer = TfidfVectorizer(max_features=1000, min_df=2, max_df=0.8, ngram_range=(1, 2), stop_words='english')\n",
    "    \n",
    "    documents = list(airline_documents.values())\n",
    "    airlines = list(airline_documents.keys())\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get top terms for each airline\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('TF-IDF Analysis: Most Distinctive Terms by Airline', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    airlines_to_plot = airlines[:4]  # Top 4 airlines\n",
    "    \n",
    "    for idx, airline in enumerate(airlines_to_plot):\n",
    "        row, col = idx // 2, idx % 2\n",
    "        \n",
    "        # Get TF-IDF scores for this airline\n",
    "        airline_idx = airlines.index(airline)\n",
    "        tfidf_scores = tfidf_matrix[airline_idx].toarray()[0]\n",
    "        \n",
    "        # Get top 10 terms\n",
    "        top_indices = tfidf_scores.argsort()[-10:][::-1]\n",
    "        top_terms = [feature_names[i] for i in top_indices]\n",
    "        top_scores = [tfidf_scores[i] for i in top_indices]\n",
    "        \n",
    "        # Calculate sentiment for each term\n",
    "        term_sentiments = []\n",
    "        for term in top_terms:\n",
    "            # Find reviews containing this term\n",
    "            term_reviews = df[(df['airline'] == airline) & (df['review_clean'].str.contains(term, na=False))]\n",
    "            if len(term_reviews) > 0:\n",
    "                term_sentiment = term_reviews['sentiment_score'].mean()\n",
    "                term_sentiments.append(term_sentiment)\n",
    "            else:\n",
    "                term_sentiments.append(0)\n",
    "        \n",
    "        # Create color map based on sentiment\n",
    "        colors = ['red' if s < -0.1 else 'green' if s > 0.1 else 'orange' for s in term_sentiments]\n",
    "        \n",
    "        # Create horizontal bar chart\n",
    "        y_pos = np.arange(len(top_terms))\n",
    "        bars = axes[row, col].barh(y_pos, top_scores, color=colors, alpha=0.7)\n",
    "        \n",
    "        axes[row, col].set_yticks(y_pos)\n",
    "        axes[row, col].set_yticklabels(top_terms)\n",
    "        axes[row, col].set_xlabel('TF-IDF Score')\n",
    "        \n",
    "        airline_name = airline.replace('_', ' ').title()\n",
    "        avg_sentiment = airline_sentiments[airline]\n",
    "        axes[row, col].set_title(f'{airline_name}\\nAvg Sentiment: {avg_sentiment:.3f}', fontweight='bold')\n",
    "        \n",
    "        # Add sentiment scores as text\n",
    "        for i, (score, sentiment) in enumerate(zip(top_scores, term_sentiments)):\n",
    "            axes[row, col].text(score + 0.001, i, f'{sentiment:.2f}', va='center', fontsize=8)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(airlines_to_plot), 4):\n",
    "        row, col = idx // 2, idx % 2\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='green', alpha=0.7, label='Positive Sentiment'),\n",
    "        Patch(facecolor='orange', alpha=0.7, label='Neutral Sentiment'),\n",
    "        Patch(facecolor='red', alpha=0.7, label='Negative Sentiment')\n",
    "    ]\n",
    "    fig.legend(handles=legend_elements, loc='lower right', bbox_to_anchor=(0.98, 0.95))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print insights\n",
    "    print(\"=== TF-IDF Insights ===\")\n",
    "    for airline in airlines_to_plot:\n",
    "        airline_idx = airlines.index(airline)\n",
    "        tfidf_scores = tfidf_matrix[airline_idx].toarray()[0]\n",
    "        top_indices = tfidf_scores.argsort()[-5:][::-1]\n",
    "        top_terms = [feature_names[i] for i in top_indices]\n",
    "        \n",
    "        print(f\"{airline.replace('_', ' ').title()} - Most Distinct Terms:\")\n",
    "        for term in top_terms:\n",
    "            print(f\"  {term}\")\n",
    "    \n",
    "    return feature_names, tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c1b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Service Aspect Sentiment Analysis\n",
    "def analyze_service_aspects_sentiment(df):\n",
    "    print(\"\\n=== SERVICE ASPECT SENTIMENT ANALYSIS ===\")\n",
    "    \n",
    "    # Define service aspects with keywords\n",
    "    service_aspects = {\n",
    "        'Crew': ['crew', 'staff', 'attendant', 'steward', 'stewardess', 'cabin crew'],\n",
    "        'Food': ['food', 'meal', 'dining', 'breakfast', 'lunch', 'dinner', 'cuisine'],\n",
    "        'Seat': ['seat', 'seating', 'comfort', 'legroom', 'space', 'chair'],\n",
    "        'Check-in': ['checkin', 'check-in', 'boarding', 'gate', 'counter'],\n",
    "        'Lounge': ['lounge', 'waiting', 'terminal', 'amenity', 'facility'],\n",
    "        'Refund': ['refund', 'money', 'compensation', 'reimburse', 'cancel']\n",
    "    }\n",
    "    \n",
    "    # Calculate aspect sentiment for each airline\n",
    "    aspect_sentiment_data = []\n",
    "    \n",
    "    for airline in df['airline'].unique():\n",
    "        airline_data = df[df['airline'] == airline]\n",
    "        \n",
    "        for aspect, keywords in service_aspects.items():\n",
    "            # Find reviews mentioning this aspect\n",
    "            aspect_reviews = airline_data[airline_data['review_clean'].str.contains('|'.join(keywords), na=False, case=False)]\n",
    "            \n",
    "            if len(aspect_reviews) > 0:\n",
    "                avg_sentiment = aspect_reviews['sentiment_score'].mean()\n",
    "                review_count = len(aspect_reviews)\n",
    "                \n",
    "                aspect_sentiment_data.append({\n",
    "                    'airline': airline, \n",
    "                    'aspect': aspect, \n",
    "                    'sentiment': avg_sentiment, \n",
    "                    'review_count': review_count\n",
    "                })\n",
    "    \n",
    "    aspect_df = pd.DataFrame(aspect_sentiment_data)\n",
    "    \n",
    "    if len(aspect_df) > 0:\n",
    "        # Create grouped bar chart\n",
    "        fig, ax = plt.subplots(figsize=(16, 8))\n",
    "        \n",
    "        # Pivot data for plotting\n",
    "        pivot_data = aspect_df.pivot(index='aspect', columns='airline', values='sentiment')\n",
    "        \n",
    "        # Create grouped bars with explicit color mapping\n",
    "        colors = {\n",
    "            'emirates': 'red',\n",
    "            'malaysia_airlines': 'blue', \n",
    "            'qatar_airways': 'maroon',\n",
    "            'singapore_airlines': 'gold'\n",
    "        }\n",
    "        \n",
    "        # Plot with manual color control\n",
    "        pivot_data.plot(\n",
    "            kind='bar', \n",
    "            ax=ax, \n",
    "            width=0.8, \n",
    "            alpha=0.8,\n",
    "            color=[colors.get(col, 'gray') for col in pivot_data.columns]\n",
    "        )\n",
    "        \n",
    "        ax.set_title('Service Aspect Sentiment Analysis by Airline', fontsize=16, fontweight='bold')\n",
    "        ax.set_xlabel('Service Aspect', fontweight='bold')\n",
    "        ax.set_ylabel('Average Sentiment Score', fontweight='bold')\n",
    "        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.7)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        ax.legend(title='Airline', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print insights\n",
    "        print(\"=== SERVICE ASPECT PERFORMANCE (Malaysia Airlines vs Competitors) ===\")\n",
    "        if 'malaysia_airlines' in pivot_data.columns:\n",
    "            mab_scores = pivot_data['malaysia_airlines'].dropna()\n",
    "            \n",
    "            for aspect in mab_scores.index:\n",
    "                mab_score = mab_scores[aspect]\n",
    "                competitor_scores = pivot_data.loc[aspect].drop('malaysia_airlines').dropna()\n",
    "                \n",
    "                if len(competitor_scores) > 0:\n",
    "                    avg_competitor = competitor_scores.mean()\n",
    "                    gap = mab_score - avg_competitor\n",
    "                    \n",
    "                    status = \"Better\" if gap > 0.05 else \"Worse\" if gap < -0.05 else \"Similar\"\n",
    "                    print(f\"  {aspect}: {mab_score:.3f} vs {avg_competitor:.3f} (Gap: {gap:+.3f}) - {status}\")\n",
    "    \n",
    "    return aspect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word Cloud\n",
    "def create_sanitized_wordcloud(df):\n",
    "    print(\"\\n=== WORD CLOUD GENERATION ===\")\n",
    "    \n",
    "    # Enhanced stopwords for cleaner results\n",
    "    enhanced_stopwords = set(stopwords.words('english'))\n",
    "    enhanced_stopwords.update({\n",
    "        'flight', 'flights', 'airline', 'airlines', 'aircraft', 'plane',\n",
    "        'airport', 'time', 'hour', 'hours', 'day', 'trip', 'travel',\n",
    "        'passenger', 'passengers', 'would', 'could', 'really', 'quite',\n",
    "        'very', 'much', 'many', 'also', 'well', 'get', 'got', 'go',\n",
    "        'went', 'come', 'came', 'take', 'took', 'one', 'two', 'way',\n",
    "        'back', 'first', 'last', 'class', 'business', 'economy',\n",
    "        'malaysia', 'singapore', 'qatar', 'emirates', 'airasia',\n",
    "        'kuala', 'lumpur', 'doha', 'dubai', 'changi', 'klia'\n",
    "    })\n",
    "    \n",
    "    def extract_sentiment_bigrams(reviews, sentiment_type='all'):\n",
    "        all_bigrams = []\n",
    "        \n",
    "        for review in reviews:\n",
    "            if pd.isna(review):\n",
    "                continue\n",
    "            \n",
    "            # Get sentiment score\n",
    "            sentiment_score = SentimentIntensityAnalyzer().polarity_scores(str(review))['compound']\n",
    "            \n",
    "            # Filter by sentiment type\n",
    "            if sentiment_type == 'positive' and sentiment_score < 0.1:\n",
    "                continue\n",
    "            elif sentiment_type == 'negative' and sentiment_score > -0.1:\n",
    "                continue\n",
    "            \n",
    "            # Extract bigrams\n",
    "            clean_text = re.sub(r'[^a-zA-Z\\s]', '', str(review).lower())\n",
    "            words = [word for word in clean_text.split() \n",
    "                    if word not in enhanced_stopwords and len(word) > 2]\n",
    "            \n",
    "            # Create bigrams\n",
    "            for i in range(len(words) - 1):\n",
    "                bigram = f\"{words[i]} {words[i+1]}\"\n",
    "                if len(bigram) > 8:  # Filter meaningful bigrams\n",
    "                    all_bigrams.append(bigram)\n",
    "        \n",
    "        return all_bigrams\n",
    "    \n",
    "    # Focus on Malaysia Airlines\n",
    "    if 'malaysia_airlines' in df['airline'].values:\n",
    "        mab_data = df[df['airline'] == 'malaysia_airlines']\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "        fig.suptitle('Malaysia Airlines: Bigram Word Clouds', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Overall word cloud\n",
    "        all_bigrams = extract_sentiment_bigrams(mab_data['review'])\n",
    "        if all_bigrams:\n",
    "            bigram_freq = Counter(all_bigrams)\n",
    "            filtered_bigrams = {bigram: count for bigram, count in bigram_freq.items() if count >= 2}\n",
    "            \n",
    "            if filtered_bigrams:\n",
    "                wordcloud = WordCloud(width=400, height=300, background_color='white', colormap='viridis', max_words=50, relative_scaling=0.6, collocations=False\n",
    "                                      ).generate_from_frequencies(filtered_bigrams)\n",
    "                \n",
    "                axes[0, 0].imshow(wordcloud, interpolation='bilinear')\n",
    "                axes[0, 0].set_title(f'Overall Customer Language\\n({len(all_bigrams)} bigrams)', fontweight='bold')\n",
    "                axes[0, 0].axis('off')\n",
    "        \n",
    "        # Positive sentiment bigrams\n",
    "        positive_bigrams = extract_sentiment_bigrams(mab_data['review'], 'positive')\n",
    "        if positive_bigrams:\n",
    "            pos_freq = Counter(positive_bigrams)\n",
    "            filtered_pos = {bigram: count for bigram, count in pos_freq.items() if count >= 2}\n",
    "            \n",
    "            if filtered_pos:\n",
    "                wordcloud_pos = WordCloud(width=400, height=300, background_color='white', colormap='Greens', max_words=40, relative_scaling=0.6, collocations=False\n",
    "                                          ).generate_from_frequencies(filtered_pos)\n",
    "                \n",
    "                axes[0, 1].imshow(wordcloud_pos, interpolation='bilinear')\n",
    "                axes[0, 1].set_title(f'Positive Experiences\\n({len(positive_bigrams)} bigrams)', fontweight='bold', color='darkgreen')\n",
    "                axes[0, 1].axis('off')\n",
    "        \n",
    "        # Negative sentiment bigrams\n",
    "        negative_bigrams = extract_sentiment_bigrams(mab_data['review'], 'negative')\n",
    "        if negative_bigrams:\n",
    "            neg_freq = Counter(negative_bigrams)\n",
    "            filtered_neg = {bigram: count for bigram, count in neg_freq.items() if count >= 2}\n",
    "            \n",
    "            if filtered_neg:\n",
    "                wordcloud_neg = WordCloud(width=400, height=300, background_color='white', colormap='Reds', max_words=40, relative_scaling=0.6, collocations=False\n",
    "                                          ).generate_from_frequencies(filtered_neg)\n",
    "                \n",
    "                axes[1, 0].imshow(wordcloud_neg, interpolation='bilinear')\n",
    "                axes[1, 0].set_title(f'Negative Experiences\\n({len(negative_bigrams)} bigrams)', fontweight='bold', color='darkred')\n",
    "                axes[1, 0].axis('off')\n",
    "        \n",
    "        # Top competitive insights\n",
    "        competitor_data = df[df['airline'].isin(['qatar_airways', 'singapore_airlines'])]\n",
    "        high_rating_reviews = competitor_data[competitor_data['overall_rating'] >= 8]['review']\n",
    "        competitor_bigrams = extract_sentiment_bigrams(high_rating_reviews, 'positive')\n",
    "        \n",
    "        if competitor_bigrams:\n",
    "            comp_freq = Counter(competitor_bigrams)\n",
    "            filtered_comp = {bigram: count for bigram, count in comp_freq.items() if count >= 3}\n",
    "            \n",
    "            if filtered_comp:\n",
    "                wordcloud_comp = WordCloud(width=400, height=300, background_color='white', colormap='Blues', max_words=40, relative_scaling=0.6, collocations=False\n",
    "                                           ).generate_from_frequencies(filtered_comp)\n",
    "                \n",
    "                axes[1, 1].imshow(wordcloud_comp, interpolation='bilinear')\n",
    "                axes[1, 1].set_title(f'Competitor Excellence\\n(Qatar/Singapore High Ratings)', fontweight='bold', color='darkblue')\n",
    "                axes[1, 1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print top insights\n",
    "        print(\"=== MALAYSIA AIRLINES - TOP POSITIVE BIGRAMS ===\")\n",
    "        if positive_bigrams:\n",
    "            for bigram, count in Counter(positive_bigrams).most_common(8):\n",
    "                print(f\"  '{bigram}': {count} mentions\")\n",
    "        \n",
    "        print(\"\\n=== MALAYSIA AIRLINES - TOP NEGATIVE BIGRAMS ===\")\n",
    "        if negative_bigrams:\n",
    "            for bigram, count in Counter(negative_bigrams).most_common(8):\n",
    "                print(f\"  '{bigram}': {count} mentions\")\n",
    "        \n",
    "        print(\"\\n=== COMPETITOR EXCELLENCE PATTERNS ===\")\n",
    "        if competitor_bigrams:\n",
    "            for bigram, count in Counter(competitor_bigrams).most_common(6):\n",
    "                print(f\"  '{bigram}': {count} mentions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca96e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Strategic Insights Summary\n",
    "def generate_strategic_insights(df):\n",
    "    print(\"\\n=== MALAYSIA AIRLINES NLP INSIGHTS SUMMARY ===\")\n",
    "    \n",
    "    if 'malaysia_airlines' in df['airline'].values:\n",
    "        mab_data = df[df['airline'] == 'malaysia_airlines']\n",
    "        \n",
    "        # Overall sentiment performance\n",
    "        mab_sentiment = mab_data['sentiment_score'].mean()\n",
    "        industry_avg = df['sentiment_score'].mean()\n",
    "        \n",
    "        print(f\"OVERALL SENTIMENT PERFORMANCE\")\n",
    "        print(f\"  Malaysia Airlines: {mab_sentiment:.3f}\")\n",
    "        print(f\"  Industry Average: {industry_avg:.3f}\")\n",
    "        print(f\"  Performance Gap: {mab_sentiment - industry_avg:+.3f}\")\n",
    "        \n",
    "        # Sentiment distribution\n",
    "        sentiment_dist = mab_data['sentiment_category'].value_counts(normalize=True) * 100\n",
    "        print(f\"\\nSENTIMENT DISTRIBUTION\")\n",
    "        for category, percentage in sentiment_dist.items():\n",
    "            print(f\"  {category}: {percentage:.1f}%\")\n",
    "        \n",
    "        # Competitive ranking\n",
    "        airline_sentiments = df.groupby('airline')['sentiment_score'].mean().sort_values(ascending=False)\n",
    "        mab_rank = list(airline_sentiments.index).index('malaysia_airlines') + 1\n",
    "        \n",
    "        print(f\"\\nCOMPETITIVE POSITION\")\n",
    "        print(f\"  Current Rank: #{mab_rank} out of {len(airline_sentiments)} airlines\")\n",
    "        \n",
    "        print(f\"\\nRANKING\")\n",
    "        for idx, (airline, sentiment) in enumerate(airline_sentiments.items(), 1):\n",
    "            print(f\"  {idx}. {airline.replace('_', ' ').title()}: {sentiment:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0bd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Execute Complete NLP\n",
    "def execute_advanced_nlp_pipeline(df):\n",
    "    \n",
    "    # Advanced preprocessing\n",
    "    df = advanced_text_preprocessing(df)\n",
    "    \n",
    "    # Bigram network analysis\n",
    "    bigram_freq, bigram_sentiment = create_bigram_network_graph(df)\n",
    "    \n",
    "    # TF-IDF analysis\n",
    "    tfidf_features, tfidf_matrix = create_tfidf_analysis(df)\n",
    "    \n",
    "    # Service aspect sentiment\n",
    "    aspect_results = analyze_service_aspects_sentiment(df)\n",
    "    \n",
    "    # Sanitized word clouds\n",
    "    create_sanitized_wordcloud(df)\n",
    "    \n",
    "    # Strategic insights\n",
    "    generate_strategic_insights(df)\n",
    "    \n",
    "    return df, bigram_freq, tfidf_features, aspect_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run NLP\n",
    "df_nlp, bigram_data, tfidf_data, service_data = execute_advanced_nlp_pipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save dataset\n",
    "# df_nlp.to_csv('airline_data_nlp.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
